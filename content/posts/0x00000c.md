+++
title = "Hexo博客SEO优化"
date = "2019-12-31"
author = [
  "Ryuchen",
]
summary = "最近因为临近寒假课程也基本都结束了想着该把这个博客再好好维护一下了啊，所以想先把这个检索的功能优化加上，毕竟我是个爱显摆的人，写了没人看多令人憋屈～"
tags = [
  "hexo",
  "Github",
  "Github Pages",
  "个人博客",
  "bash",
  "SEO"
]
series = [
    "个人生活"
]
categories = [
  "博客搭建"
]
images = [
    "markdown-syntax.jpg",
]
share = true
+++

## Robots.txt

 首先得先知道这个robots.txt是个什么玩意儿(这话怎么听着这么智障，我本来就知道还得着么写= =|||，不过没办法嘛博客，不写的详细点怎么对得起码字的自己0.0)
 
 这里引用一下wiki百科的说明：
 
 > robots.txt（统一小写）是一种存放于网站根目录下的ASCII编码的文本文件，它通常告诉网络搜索引擎的漫游器（又称网络蜘蛛），此网站中的哪些内容是不应被搜索引擎的漫游器获取的，哪些是可以被漫游器获取的。因为一些系统中的URL是大小写敏感的，所以robots.txt的文件名应统一为小写。robots.txt应放置于网站的根目录下。如果想单独定义搜索引擎的漫游器访问子目录时的行为，那么可以将自定的设置合并到根目录下的robots.txt，或者使用robots元数据（Metadata，又称元数据）。

> robots.txt协议并不是一个规范，而只是约定俗成的，所以并不能保证网站的隐私。注意robots.txt是用字符串比较来确定是否获取URL，所以目录末尾有与没有斜杠“/”表示的是不同的URL。robots.txt允许使用类似"Disallow: *.gif"这样的通配符。

## 如何配置

在hexo 根目录下的 public 目录下新建一个robots.txt文件，写上特定的内容就行了，具体格式可以百度，谷歌搜索一下，也有一些网站能够帮忙生成，这里我自己的博客内容如下：

```
User-agent: *
Allow: /
Allow: /archives/
Allow: /categories/
Allow: /tags/
Allow: /about/
Disallow: /vendors/
Disallow: /css/
Disallow: /fancybox/
Sitemap: https://ryuchen.github.io/sitemap.xml
Sitemap: https://ryuchen.github.io/baidu_sitemap.xml
```

## sitemap.xml

sitemap即网站地图，它的作用在于便于搜索引擎更加智能地抓取网站。最简单和常见的sitemap形式，是XML文件，在其中列出网站中的网址以及关于每个网址的其他元数据（上次更新时间、更新的频率及相对其他网址重要程度等）。

如果你的项目没有这两个东西的话，需要安装一下对应的hexo插件：

```bash
npm install hexo-generator-sitemap --save
npm install hexo-generator-baidu-sitemap --save
```

只安装插件并不行，还需要在配置文件（`_config.yml`）中添加相关的配置：

```xml
sitemap:
  path: sitemap.xml
baidusitemap:
  path: baidusitemap.xml
```

## 发布

重新运行
```bash
hexo clean && hexo g && hexo d
```
这样就完成了博客的检索，一般一天之内就能够通过检索查到你的博客内容了。
